{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if i >= 3 and X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if i >= 8 and X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            X[i] = 0\n",
    "        else:\n",
    "            X[i] = 1\n",
    "    return X\n",
    "def gen_data_sequence(examples=50000, num_seq = 4, num_steps = 20):\n",
    "    X = np.zeros([examples, num_seq, num_steps])\n",
    "    for i in range(examples):\n",
    "        for j in range(num_seq*num_steps):\n",
    "            jj, kk = divmod(j,num_steps)\n",
    "            if ((j + 1) % num_steps == 0):\n",
    "                X[i,jj, kk] = 2 # EOL character\n",
    "                continue\n",
    "            threshold = 0.5\n",
    "            j3, k3 = divmod(j - 3, num_steps)\n",
    "            j8, k8 = divmod(j - 8, num_steps)\n",
    "            if j >= 3 and X[i,j3, k3] == 1:\n",
    "                threshold += 0.5\n",
    "            if j >= 8 and X[i,j8, k8] == 1:\n",
    "                threshold -= 0.25\n",
    "            if np.random.rand() > threshold:\n",
    "                X[i,jj,kk] = 0\n",
    "            else:\n",
    "                X[i,jj,kk] = 1\n",
    "    return X\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"Generates a batch iterator for a dataset.\"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    if len(data) % batch_size == 0:\n",
    "        num_batches_per_epoch = int(len(data) / batch_size)\n",
    "    else:\n",
    "        num_batches_per_epoch = int(len(data) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(np.arange(data_size))\n",
    "        else:\n",
    "            indices = np.arange(data_size)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            if end_index - start_index != batch_size:\n",
    "                continue\n",
    "            yield data[indices[start_index:end_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "examples=50000\n",
    "num_steps = 20\n",
    "num_seq = 4\n",
    "n_hidden_enc = 4\n",
    "n_hidden_con = 5\n",
    "n_hidden_dec = 8\n",
    "batch_size = 200\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_encoders(x_sequences, n_hidden_enc, init_state):\n",
    "    # x_sequences - list(num_seq - 1 *[batch_size, num_steps, 1-hot])\n",
    "    with tf.variable_scope('encoder') as enc_scope:\n",
    "        cell = tf.nn.rnn_cell.GRUCell(n_hidden_enc)\n",
    "        final_states = []\n",
    "        for x_seq in x_sequences:\n",
    "            x = tf.unpack(x_seq, axis = 1) # list(num_steps * [batch_size, 1-hot])\n",
    "            _, final_state = tf.nn.rnn(cell, x, initial_state = init_state, scope = enc_scope)\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            final_states.append(final_state)\n",
    "    return final_states\n",
    "\n",
    "def generate_decoders(y_sequences, n_hidden_dec, init_states):\n",
    "    # y_sequences - list(num_seq - 1 *[batch_size, num_steps, 1-hot])\n",
    "    with tf.variable_scope('decoder') as dec_scope:\n",
    "        cell = tf.nn.rnn_cell.GRUCell(n_hidden_dec)\n",
    "        outputs_list = []\n",
    "        for (y_seq, init_s) in zip(y_sequences, init_states):\n",
    "            y = tf.unpack(y_seq, axis = 1) # list(num_steps * [batch_size, 1-hot])\n",
    "            y = [tf.zeros([batch_size, 3])] + y \n",
    "            outputs, _ = tf.nn.seq2seq.rnn_decoder(y[:-1], init_s, cell, scope = dec_scope)\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            outputs_list.append(outputs)\n",
    "    return outputs_list\n",
    "\n",
    "## Build graph ##\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# input\n",
    "sequence_input = tf.placeholder(tf.int32, [batch_size, num_seq, num_steps])\n",
    "\n",
    "sequence = tf.one_hot(sequence_input, 3) # [batch_size, num_seq, num_steps, 1-hot]\n",
    "sequence = tf.unpack(sequence, axis=1) # list(num_seq * [batch_size, num_steps, 1-hot])\n",
    "x = sequence[:-1] # list(num_seq - 1 *[batch_size, num_steps, 1-hot]) \n",
    "y = sequence[1:] # list(num_seq - 1 *[batch_size, num_steps, 1-hot])\n",
    "\n",
    "#x = tf.unpack(x, axis = 1) # list(num_steps * [batch_size, 1-hot])\n",
    "#y = tf.unpack(y, axis = 1) # list(num_steps * [batch_size, 1-hot])\n",
    "\n",
    "# Encoder\n",
    "init_state_enc = tf.zeros([batch_size, n_hidden_enc])\n",
    "final_states_enc = generate_encoders(x, n_hidden_enc, init_state_enc)\n",
    "\n",
    "# Context\n",
    "init_state_con = tf.zeros([batch_size, n_hidden_con])\n",
    "context_cell = tf.nn.rnn_cell.GRUCell(n_hidden_con)\n",
    "output_context, _ = tf.nn.rnn(context_cell, final_states_enc, initial_state = init_state_con)\n",
    "\n",
    "# Context to decoder\n",
    "W_con_to_dec = tf.get_variable('W_con_to_dec', [n_hidden_con, n_hidden_dec])\n",
    "init_state_dec = [tf.matmul(fs, W_con_to_dec) for fs in output_context]\n",
    "\n",
    "# Decoder\n",
    "output_dec = generate_decoders(y, n_hidden_dec, init_state_dec)\n",
    "output_dec = sum(output_dec,[])\n",
    "\n",
    "# To output\n",
    "W_out = tf.get_variable('W_out', [n_hidden_dec, 3])\n",
    "b_out = tf.get_variable('b_out', [3])\n",
    "\n",
    "logits = [tf.matmul(o, W_out) + b_out for o in output_dec]\n",
    "predictions = [tf.nn.softmax(l) for l in logits]\n",
    "y = sum([tf.unpack(yv, axis = 1) for yv in y],[])\n",
    "\n",
    "y = [tf.argmax(yv, 1) for yv in y]\n",
    "loss_weights = [tf.ones([batch_size]) for i in range(num_steps*(num_seq- 1))]\n",
    "\n",
    "# Cost and training\n",
    "losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y, loss_weights)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = gen_data_sequence(examples=examples, num_seq=num_seq, num_steps=num_steps)\n",
    "#X = np.reshape(X, [-1, num_seq, num_steps])\n",
    "\n",
    "batches = batch_iter(list(X), batch_size = batch_size, num_epochs = 3)\n",
    "\n",
    "acc_loss = 0\n",
    "timer = datetime.now()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for i, batch in enumerate(batches):\n",
    "        x_batch = np.array(batch)\n",
    "        feed_dict = {sequence_input : x_batch,\n",
    "                     }\n",
    "        loss, _ = sess.run([total_loss, train_step], feed_dict=feed_dict)\n",
    "        acc_loss += loss\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(acc_loss/100)\n",
    "            acc_loss = 0\n",
    "timer = datetime.now() - timer\n",
    "print(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = gen_data_sequence(examples=1000, num_seq=4, num_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
